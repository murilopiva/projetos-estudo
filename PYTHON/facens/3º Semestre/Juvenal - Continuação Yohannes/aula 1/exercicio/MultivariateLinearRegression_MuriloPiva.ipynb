{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Regressão Linear Multivariada</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando as bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(precision=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implementação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funções a serem implementadas no exercício:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##############################################################################################\n",
    "# Esta função deve inicializar os parâmetros do modelo de Regressão Linear, sendo o vetor \n",
    "# de coeficientes = {m1, m2, m3, ..., mn} do tamanho da quantidade de atributos (n) e o bias um \n",
    "# valor escalar. Os resultados devem ser retornados em forma de tupla: (weights, bias).\n",
    "# ##############################################################################################\n",
    "#passo inicial zero\n",
    "\n",
    "\n",
    "# na dúvida, rever o jupyternb de algebra linear\n",
    "def initialize_weights_bias(x): #matriz x de entrada\n",
    "    try:\n",
    "        (nlines, ncolumns) = x.shape\n",
    "    except:    \n",
    "        ncolumns = 1\n",
    "\n",
    "    # AJUSTE O TRECHO ABAIXO\n",
    "    # não inicializar com zeros (0)\n",
    "    #inicializar com 1 ou nº randomico\n",
    "    # --------------------------\n",
    "    #weights = np.random.randint(1,10, size=ncolumns)\n",
    "    weights = np.ones(shape = ncolumns)    \n",
    "\n",
    "    weights = weights.reshape(len(weights), 1)\n",
    "    \n",
    "    #bias = np.random.randint(1,10)\n",
    "    bias = 0 #o bias pode ser zero\n",
    "    # --------------------------\n",
    "\n",
    "    return (weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##############################################################################################\n",
    "# Esta funcao deve computar a predicao para cada um dos m registros em x, baseado nos coeficien-\n",
    "# tes em weights e no bias. Utilize a funcao (1) dos slides.\n",
    "# ##############################################################################################\n",
    "def predict(x, weights, bias): #weights é o BETA da fórmula e o bias é o a da fórmula (coeficiente aditivo)\n",
    "    # CALCULE ABAIXO\n",
    "    # --------------------------\n",
    "    #return x.dot(... multiplicar x X weights e somar 0 bias\n",
    "    return x.dot(weights) + bias\n",
    "    # --------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##############################################################################################\n",
    "# Coloque aqui o trecho de código que vai calcular o custo gerado pelo modelo.\n",
    "# A variável \"y\" representa o dado real ocorrido, enquanto \"yhat\" indica as predições.\n",
    "# O custo deve ser calculado pelo Método de Mínimos Quadrados. Pode-se assumir que os arrays\n",
    "# fornecidos como parâmetro são numpy.array, então esta função pode ser implementada com\n",
    "# 3 linhas de código. Use a funcao (3) dos slides.\n",
    "# ##############################################################################################\n",
    "def cost_function(y, yhat): #y é o valor sugerido, valor conhecido, valor REAL; yhat (y^) é o predito\n",
    "    m = len(y)\n",
    "    \n",
    "    # CALCULE J ABAIXO\n",
    "    loss = y - yhat\n",
    "    lossSqr = np.square(loss) # com loss ** 2 dava erro quando é multivariado\n",
    "    # --------------------------\n",
    "    J = np.sum(lossSqr)  / m\n",
    "    # --------------------------\n",
    "    \n",
    "    #implementação do somatorio da perda\n",
    "    \n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##############################################################################################\n",
    "# O codigo abaixo apresenta alguns trechos destacados, os quais precisam ser corrigos.\n",
    "# ##############################################################################################\n",
    "def batch_gradient_descent(x, y, alpha=0.005, iterations=50):\n",
    "    m = len(y)\n",
    "    weights, bias = initialize_weights_bias(x)\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        yhat = predict(x, weights, bias)\n",
    "        #(z, m) = yhat.shape\n",
    "        yhat = yhat.reshape(m,1)\n",
    "        \n",
    "        loss = y - yhat\n",
    "        wgradient = (-2/m) *  x.T.dot(loss)#função 4\n",
    "        \n",
    "        # ATUALIZE OS COEFICIENTES\n",
    "        # --------------------------\n",
    "        weights = weights - wgradient * alpha\n",
    "        # --------------------------\n",
    "        \n",
    "        # CALCULE A DERIVADA PARCIAL \n",
    "        #  DA FUNCAO J EM RELACAO \n",
    "        #  AO BIAS. USE A FUNCAO (5)\n",
    "        #  DOS SLIDES.\n",
    "        # --------------------------        \n",
    "        bgradient = (-2/m) * np.sum(loss)\n",
    "        # --------------------------\n",
    "        \n",
    "        bias = bias - alpha * bgradient\n",
    "\n",
    "        nyhat = predict(x, weights, bias)\n",
    "        \n",
    "        cost = cost_function(y, nyhat)\n",
    "        print(\"Gradient descendent iteration %d. Cost = %.8f.\" %(iteration, cost))\n",
    "\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Teste aqui seu código:\n",
    "----------------------------------------------------------------------------------------------------------------------------\n",
    "1. Inicialização de pesos. Resultados esperados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (array([[1.]]), 0)\n",
    "# (array([[1.],\n",
    "#        [1.]]), 0)\n",
    "# (array([[1.],\n",
    "#        [1.],\n",
    "#        [1.],\n",
    "#        [1.]]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[1.]]), 0)\n",
      "(array([[1.],\n",
      "       [1.]]), 0)\n",
      "(array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), 0)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.array([0, 0, 0, 0, 0, 0])\n",
    "x2 = np.matrix('1 2; 3 4')\n",
    "x3 = np.matrix('11 12 13 14; 21 22 23 24; 31 32 33 34')\n",
    "\n",
    "parameters = initialize_weights_bias(x1)\n",
    "print(parameters)\n",
    "\n",
    "parameters = initialize_weights_bias(x2)\n",
    "print(parameters)\n",
    "\n",
    "parameters = initialize_weights_bias(x3)\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------\n",
    "2. Calcule a predição baseado no vetor x e nos parametros.  Resultados esperados:\n",
    "\n",
    "1.0\n",
    "\n",
    "[[3.5 6.5]]\n",
    "\n",
    "[[13.5 23.5 33.5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[3.5 6.5]]\n",
      "[[13.5 23.5 33.5]]\n"
     ]
    }
   ],
   "source": [
    "w1 = np.array([0.5])\n",
    "w2 = np.array([0.5, 1])\n",
    "w3 = np.array([0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "b1 = 1\n",
    "b2 = 1\n",
    "b3 = 1\n",
    "\n",
    "yhat1 = predict(np.array([0]), w1, b1)\n",
    "yhat2 = predict(x2, w2, b2)\n",
    "yhat3 = predict(x3, w3, b3)\n",
    "\n",
    "print(yhat1)\n",
    "print(yhat2)\n",
    "print(yhat3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------\n",
    "3. Teste a funcao de custo. Resultados esperados:\n",
    "\n",
    "    5.166666666666667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.166666666666667\n"
     ]
    }
   ],
   "source": [
    "y = np.array([2, 2, 2, 2, 2, 2])\n",
    "yhat = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "cost = cost_function(y=y, yhat=yhat)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------\n",
    "4. Otimize os pesos para os exemplos abaixo. Resultados esperados:\n",
    "\n",
    ">> Univariada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descendent iteration 0. Cost = 0.38000000.\n",
      "Gradient descendent iteration 1. Cost = 0.02360000.\n",
      "Gradient descendent iteration 2. Cost = 0.01128800.\n",
      "Gradient descendent iteration 3. Cost = 0.01052336.\n",
      "Gradient descendent iteration 4. Cost = 0.01015763.\n",
      "Gradient descendent iteration 5. Cost = 0.00981667.\n",
      "Gradient descendent iteration 6. Cost = 0.00948756.\n",
      "Gradient descendent iteration 7. Cost = 0.00916950.\n",
      "Gradient descendent iteration 8. Cost = 0.00886211.\n",
      "Gradient descendent iteration 9. Cost = 0.00856502.\n",
      "Gradient descendent iteration 10. Cost = 0.00827788.\n",
      "Gradient descendent iteration 11. Cost = 0.00800038.\n",
      "Gradient descendent iteration 12. Cost = 0.00773217.\n",
      "Gradient descendent iteration 13. Cost = 0.00747296.\n",
      "Gradient descendent iteration 14. Cost = 0.00722244.\n",
      "Gradient descendent iteration 15. Cost = 0.00698031.\n",
      "Gradient descendent iteration 16. Cost = 0.00674631.\n",
      "Gradient descendent iteration 17. Cost = 0.00652015.\n",
      "Gradient descendent iteration 18. Cost = 0.00630157.\n",
      "Gradient descendent iteration 19. Cost = 0.00609031.\n",
      "Gradient descendent iteration 20. Cost = 0.00588614.\n",
      "Gradient descendent iteration 21. Cost = 0.00568882.\n",
      "Gradient descendent iteration 22. Cost = 0.00549810.\n",
      "Gradient descendent iteration 23. Cost = 0.00531379.\n",
      "Gradient descendent iteration 24. Cost = 0.00513565.\n",
      "Gradient descendent iteration 25. Cost = 0.00496348.\n",
      "Gradient descendent iteration 26. Cost = 0.00479709.\n",
      "Gradient descendent iteration 27. Cost = 0.00463627.\n",
      "Gradient descendent iteration 28. Cost = 0.00448084.\n",
      "Gradient descendent iteration 29. Cost = 0.00433063.\n",
      "Gradient descendent iteration 30. Cost = 0.00418545.\n",
      "Gradient descendent iteration 31. Cost = 0.00404514.\n",
      "Gradient descendent iteration 32. Cost = 0.00390953.\n",
      "Gradient descendent iteration 33. Cost = 0.00377847.\n",
      "Gradient descendent iteration 34. Cost = 0.00365180.\n",
      "Gradient descendent iteration 35. Cost = 0.00352938.\n",
      "Gradient descendent iteration 36. Cost = 0.00341106.\n",
      "Gradient descendent iteration 37. Cost = 0.00329671.\n",
      "Gradient descendent iteration 38. Cost = 0.00318619.\n",
      "Gradient descendent iteration 39. Cost = 0.00307937.\n",
      "Gradient descendent iteration 40. Cost = 0.00297614.\n",
      "Gradient descendent iteration 41. Cost = 0.00287637.\n",
      "Gradient descendent iteration 42. Cost = 0.00277994.\n",
      "Gradient descendent iteration 43. Cost = 0.00268675.\n",
      "Gradient descendent iteration 44. Cost = 0.00259668.\n",
      "Gradient descendent iteration 45. Cost = 0.00250963.\n",
      "Gradient descendent iteration 46. Cost = 0.00242550.\n",
      "Gradient descendent iteration 47. Cost = 0.00234418.\n",
      "Gradient descendent iteration 48. Cost = 0.00226560.\n",
      "Gradient descendent iteration 49. Cost = 0.00218965.\n",
      "Gradient descendent iteration 50. Cost = 0.00211624.\n",
      "Gradient descendent iteration 51. Cost = 0.00204530.\n",
      "Gradient descendent iteration 52. Cost = 0.00197673.\n",
      "Gradient descendent iteration 53. Cost = 0.00191046.\n",
      "Gradient descendent iteration 54. Cost = 0.00184642.\n",
      "Gradient descendent iteration 55. Cost = 0.00178452.\n",
      "Gradient descendent iteration 56. Cost = 0.00172469.\n",
      "Gradient descendent iteration 57. Cost = 0.00166687.\n",
      "Gradient descendent iteration 58. Cost = 0.00161099.\n",
      "Gradient descendent iteration 59. Cost = 0.00155699.\n",
      "Gradient descendent iteration 60. Cost = 0.00150479.\n",
      "Gradient descendent iteration 61. Cost = 0.00145435.\n",
      "Gradient descendent iteration 62. Cost = 0.00140559.\n",
      "Gradient descendent iteration 63. Cost = 0.00135847.\n",
      "Gradient descendent iteration 64. Cost = 0.00131293.\n",
      "Gradient descendent iteration 65. Cost = 0.00126891.\n",
      "Gradient descendent iteration 66. Cost = 0.00122637.\n",
      "Gradient descendent iteration 67. Cost = 0.00118526.\n",
      "Gradient descendent iteration 68. Cost = 0.00114553.\n",
      "Gradient descendent iteration 69. Cost = 0.00110713.\n",
      "Gradient descendent iteration 70. Cost = 0.00107001.\n",
      "Gradient descendent iteration 71. Cost = 0.00103414.\n",
      "Gradient descendent iteration 72. Cost = 0.00099947.\n",
      "Gradient descendent iteration 73. Cost = 0.00096596.\n",
      "Gradient descendent iteration 74. Cost = 0.00093358.\n",
      "Gradient descendent iteration 75. Cost = 0.00090228.\n",
      "Gradient descendent iteration 76. Cost = 0.00087204.\n",
      "Gradient descendent iteration 77. Cost = 0.00084280.\n",
      "Gradient descendent iteration 78. Cost = 0.00081455.\n",
      "Gradient descendent iteration 79. Cost = 0.00078724.\n",
      "Gradient descendent iteration 80. Cost = 0.00076085.\n",
      "Gradient descendent iteration 81. Cost = 0.00073534.\n",
      "Gradient descendent iteration 82. Cost = 0.00071069.\n",
      "Gradient descendent iteration 83. Cost = 0.00068687.\n",
      "Gradient descendent iteration 84. Cost = 0.00066384.\n",
      "Gradient descendent iteration 85. Cost = 0.00064159.\n",
      "Gradient descendent iteration 86. Cost = 0.00062008.\n",
      "Gradient descendent iteration 87. Cost = 0.00059929.\n",
      "Gradient descendent iteration 88. Cost = 0.00057920.\n",
      "Gradient descendent iteration 89. Cost = 0.00055978.\n",
      "Gradient descendent iteration 90. Cost = 0.00054102.\n",
      "Gradient descendent iteration 91. Cost = 0.00052288.\n",
      "Gradient descendent iteration 92. Cost = 0.00050535.\n",
      "Gradient descendent iteration 93. Cost = 0.00048841.\n",
      "Gradient descendent iteration 94. Cost = 0.00047204.\n",
      "Gradient descendent iteration 95. Cost = 0.00045621.\n",
      "Gradient descendent iteration 96. Cost = 0.00044092.\n",
      "Gradient descendent iteration 97. Cost = 0.00042614.\n",
      "Gradient descendent iteration 98. Cost = 0.00041185.\n",
      "Gradient descendent iteration 99. Cost = 0.00039804.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Coeficientes:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1.9870472]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Bias:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.04676370377164412"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# univariada\n",
    "x1 = np.array([1, 2, 3, 4, 5]).reshape(5,1)\n",
    "y1 = np.array([2, 4, 6, 8, 10]).reshape(5,1)\n",
    "\n",
    "weights1, b1 = batch_gradient_descent(x1, y1, alpha=0.05, iterations=100)\n",
    "display(\"Coeficientes:\")\n",
    "display(weights1)\n",
    "display(\"Bias:\")\n",
    "display(b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------\n",
    ">> Multivariada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descendent iteration 0. Cost = 0.43500000.\n",
      "Gradient descendent iteration 1. Cost = 0.23922500.\n",
      "Gradient descendent iteration 2. Cost = 0.16956287.\n",
      "Gradient descendent iteration 3. Cost = 0.14303360.\n",
      "Gradient descendent iteration 4. Cost = 0.13131229.\n",
      "Gradient descendent iteration 5. Cost = 0.12472862.\n",
      "Gradient descendent iteration 6. Cost = 0.11997936.\n",
      "Gradient descendent iteration 7. Cost = 0.11593457.\n",
      "Gradient descendent iteration 8. Cost = 0.11220610.\n",
      "Gradient descendent iteration 9. Cost = 0.10865913.\n",
      "Gradient descendent iteration 10. Cost = 0.10524532.\n",
      "Gradient descendent iteration 11. Cost = 0.10194594.\n",
      "Gradient descendent iteration 12. Cost = 0.09875245.\n",
      "Gradient descendent iteration 13. Cost = 0.09565983.\n",
      "Gradient descendent iteration 14. Cost = 0.09266435.\n",
      "Gradient descendent iteration 15. Cost = 0.08976277.\n",
      "Gradient descendent iteration 16. Cost = 0.08695208.\n",
      "Gradient descendent iteration 17. Cost = 0.08422940.\n",
      "Gradient descendent iteration 18. Cost = 0.08159199.\n",
      "Gradient descendent iteration 19. Cost = 0.07903716.\n",
      "Gradient descendent iteration 20. Cost = 0.07656233.\n",
      "Gradient descendent iteration 21. Cost = 0.07416499.\n",
      "Gradient descendent iteration 22. Cost = 0.07184271.\n",
      "Gradient descendent iteration 23. Cost = 0.06959316.\n",
      "Gradient descendent iteration 24. Cost = 0.06741404.\n",
      "Gradient descendent iteration 25. Cost = 0.06530315.\n",
      "Gradient descendent iteration 26. Cost = 0.06325836.\n",
      "Gradient descendent iteration 27. Cost = 0.06127760.\n",
      "Gradient descendent iteration 28. Cost = 0.05935886.\n",
      "Gradient descendent iteration 29. Cost = 0.05750020.\n",
      "Gradient descendent iteration 30. Cost = 0.05569974.\n",
      "Gradient descendent iteration 31. Cost = 0.05395566.\n",
      "Gradient descendent iteration 32. Cost = 0.05226618.\n",
      "Gradient descendent iteration 33. Cost = 0.05062961.\n",
      "Gradient descendent iteration 34. Cost = 0.04904428.\n",
      "Gradient descendent iteration 35. Cost = 0.04750860.\n",
      "Gradient descendent iteration 36. Cost = 0.04602099.\n",
      "Gradient descendent iteration 37. Cost = 0.04457997.\n",
      "Gradient descendent iteration 38. Cost = 0.04318407.\n",
      "Gradient descendent iteration 39. Cost = 0.04183188.\n",
      "Gradient descendent iteration 40. Cost = 0.04052203.\n",
      "Gradient descendent iteration 41. Cost = 0.03925320.\n",
      "Gradient descendent iteration 42. Cost = 0.03802409.\n",
      "Gradient descendent iteration 43. Cost = 0.03683347.\n",
      "Gradient descendent iteration 44. Cost = 0.03568013.\n",
      "Gradient descendent iteration 45. Cost = 0.03456290.\n",
      "Gradient descendent iteration 46. Cost = 0.03348066.\n",
      "Gradient descendent iteration 47. Cost = 0.03243231.\n",
      "Gradient descendent iteration 48. Cost = 0.03141678.\n",
      "Gradient descendent iteration 49. Cost = 0.03043305.\n",
      "Gradient descendent iteration 50. Cost = 0.02948012.\n",
      "Gradient descendent iteration 51. Cost = 0.02855703.\n",
      "Gradient descendent iteration 52. Cost = 0.02766285.\n",
      "Gradient descendent iteration 53. Cost = 0.02679666.\n",
      "Gradient descendent iteration 54. Cost = 0.02595760.\n",
      "Gradient descendent iteration 55. Cost = 0.02514481.\n",
      "Gradient descendent iteration 56. Cost = 0.02435746.\n",
      "Gradient descendent iteration 57. Cost = 0.02359478.\n",
      "Gradient descendent iteration 58. Cost = 0.02285597.\n",
      "Gradient descendent iteration 59. Cost = 0.02214030.\n",
      "Gradient descendent iteration 60. Cost = 0.02144704.\n",
      "Gradient descendent iteration 61. Cost = 0.02077548.\n",
      "Gradient descendent iteration 62. Cost = 0.02012495.\n",
      "Gradient descendent iteration 63. Cost = 0.01949480.\n",
      "Gradient descendent iteration 64. Cost = 0.01888437.\n",
      "Gradient descendent iteration 65. Cost = 0.01829306.\n",
      "Gradient descendent iteration 66. Cost = 0.01772026.\n",
      "Gradient descendent iteration 67. Cost = 0.01716540.\n",
      "Gradient descendent iteration 68. Cost = 0.01662791.\n",
      "Gradient descendent iteration 69. Cost = 0.01610726.\n",
      "Gradient descendent iteration 70. Cost = 0.01560290.\n",
      "Gradient descendent iteration 71. Cost = 0.01511434.\n",
      "Gradient descendent iteration 72. Cost = 0.01464107.\n",
      "Gradient descendent iteration 73. Cost = 0.01418263.\n",
      "Gradient descendent iteration 74. Cost = 0.01373854.\n",
      "Gradient descendent iteration 75. Cost = 0.01330835.\n",
      "Gradient descendent iteration 76. Cost = 0.01289164.\n",
      "Gradient descendent iteration 77. Cost = 0.01248797.\n",
      "Gradient descendent iteration 78. Cost = 0.01209695.\n",
      "Gradient descendent iteration 79. Cost = 0.01171816.\n",
      "Gradient descendent iteration 80. Cost = 0.01135124.\n",
      "Gradient descendent iteration 81. Cost = 0.01099581.\n",
      "Gradient descendent iteration 82. Cost = 0.01065151.\n",
      "Gradient descendent iteration 83. Cost = 0.01031798.\n",
      "Gradient descendent iteration 84. Cost = 0.00999490.\n",
      "Gradient descendent iteration 85. Cost = 0.00968194.\n",
      "Gradient descendent iteration 86. Cost = 0.00937878.\n",
      "Gradient descendent iteration 87. Cost = 0.00908511.\n",
      "Gradient descendent iteration 88. Cost = 0.00880063.\n",
      "Gradient descendent iteration 89. Cost = 0.00852506.\n",
      "Gradient descendent iteration 90. Cost = 0.00825812.\n",
      "Gradient descendent iteration 91. Cost = 0.00799954.\n",
      "Gradient descendent iteration 92. Cost = 0.00774906.\n",
      "Gradient descendent iteration 93. Cost = 0.00750642.\n",
      "Gradient descendent iteration 94. Cost = 0.00727138.\n",
      "Gradient descendent iteration 95. Cost = 0.00704369.\n",
      "Gradient descendent iteration 96. Cost = 0.00682314.\n",
      "Gradient descendent iteration 97. Cost = 0.00660949.\n",
      "Gradient descendent iteration 98. Cost = 0.00640253.\n",
      "Gradient descendent iteration 99. Cost = 0.00620206.\n",
      "Gradient descendent iteration 100. Cost = 0.00600786.\n",
      "Gradient descendent iteration 101. Cost = 0.00581974.\n",
      "Gradient descendent iteration 102. Cost = 0.00563751.\n",
      "Gradient descendent iteration 103. Cost = 0.00546098.\n",
      "Gradient descendent iteration 104. Cost = 0.00528999.\n",
      "Gradient descendent iteration 105. Cost = 0.00512435.\n",
      "Gradient descendent iteration 106. Cost = 0.00496389.\n",
      "Gradient descendent iteration 107. Cost = 0.00480846.\n",
      "Gradient descendent iteration 108. Cost = 0.00465790.\n",
      "Gradient descendent iteration 109. Cost = 0.00451205.\n",
      "Gradient descendent iteration 110. Cost = 0.00437077.\n",
      "Gradient descendent iteration 111. Cost = 0.00423391.\n",
      "Gradient descendent iteration 112. Cost = 0.00410133.\n",
      "Gradient descendent iteration 113. Cost = 0.00397291.\n",
      "Gradient descendent iteration 114. Cost = 0.00384851.\n",
      "Gradient descendent iteration 115. Cost = 0.00372801.\n",
      "Gradient descendent iteration 116. Cost = 0.00361127.\n",
      "Gradient descendent iteration 117. Cost = 0.00349820.\n",
      "Gradient descendent iteration 118. Cost = 0.00338866.\n",
      "Gradient descendent iteration 119. Cost = 0.00328255.\n",
      "Gradient descendent iteration 120. Cost = 0.00317977.\n",
      "Gradient descendent iteration 121. Cost = 0.00308020.\n",
      "Gradient descendent iteration 122. Cost = 0.00298376.\n",
      "Gradient descendent iteration 123. Cost = 0.00289033.\n",
      "Gradient descendent iteration 124. Cost = 0.00279982.\n",
      "Gradient descendent iteration 125. Cost = 0.00271216.\n",
      "Gradient descendent iteration 126. Cost = 0.00262723.\n",
      "Gradient descendent iteration 127. Cost = 0.00254497.\n",
      "Gradient descendent iteration 128. Cost = 0.00246528.\n",
      "Gradient descendent iteration 129. Cost = 0.00238809.\n",
      "Gradient descendent iteration 130. Cost = 0.00231331.\n",
      "Gradient descendent iteration 131. Cost = 0.00224087.\n",
      "Gradient descendent iteration 132. Cost = 0.00217071.\n",
      "Gradient descendent iteration 133. Cost = 0.00210274.\n",
      "Gradient descendent iteration 134. Cost = 0.00203690.\n",
      "Gradient descendent iteration 135. Cost = 0.00197312.\n",
      "Gradient descendent iteration 136. Cost = 0.00191133.\n",
      "Gradient descendent iteration 137. Cost = 0.00185149.\n",
      "Gradient descendent iteration 138. Cost = 0.00179351.\n",
      "Gradient descendent iteration 139. Cost = 0.00173735.\n",
      "Gradient descendent iteration 140. Cost = 0.00168295.\n",
      "Gradient descendent iteration 141. Cost = 0.00163025.\n",
      "Gradient descendent iteration 142. Cost = 0.00157921.\n",
      "Gradient descendent iteration 143. Cost = 0.00152976.\n",
      "Gradient descendent iteration 144. Cost = 0.00148186.\n",
      "Gradient descendent iteration 145. Cost = 0.00143546.\n",
      "Gradient descendent iteration 146. Cost = 0.00139051.\n",
      "Gradient descendent iteration 147. Cost = 0.00134697.\n",
      "Gradient descendent iteration 148. Cost = 0.00130479.\n",
      "Gradient descendent iteration 149. Cost = 0.00126394.\n",
      "Gradient descendent iteration 150. Cost = 0.00122436.\n",
      "Gradient descendent iteration 151. Cost = 0.00118602.\n",
      "Gradient descendent iteration 152. Cost = 0.00114889.\n",
      "Gradient descendent iteration 153. Cost = 0.00111291.\n",
      "Gradient descendent iteration 154. Cost = 0.00107806.\n",
      "Gradient descendent iteration 155. Cost = 0.00104431.\n",
      "Gradient descendent iteration 156. Cost = 0.00101161.\n",
      "Gradient descendent iteration 157. Cost = 0.00097993.\n",
      "Gradient descendent iteration 158. Cost = 0.00094925.\n",
      "Gradient descendent iteration 159. Cost = 0.00091953.\n",
      "Gradient descendent iteration 160. Cost = 0.00089073.\n",
      "Gradient descendent iteration 161. Cost = 0.00086284.\n",
      "Gradient descendent iteration 162. Cost = 0.00083583.\n",
      "Gradient descendent iteration 163. Cost = 0.00080965.\n",
      "Gradient descendent iteration 164. Cost = 0.00078430.\n",
      "Gradient descendent iteration 165. Cost = 0.00075974.\n",
      "Gradient descendent iteration 166. Cost = 0.00073595.\n",
      "Gradient descendent iteration 167. Cost = 0.00071291.\n",
      "Gradient descendent iteration 168. Cost = 0.00069059.\n",
      "Gradient descendent iteration 169. Cost = 0.00066896.\n",
      "Gradient descendent iteration 170. Cost = 0.00064802.\n",
      "Gradient descendent iteration 171. Cost = 0.00062773.\n",
      "Gradient descendent iteration 172. Cost = 0.00060807.\n",
      "Gradient descendent iteration 173. Cost = 0.00058903.\n",
      "Gradient descendent iteration 174. Cost = 0.00057059.\n",
      "Gradient descendent iteration 175. Cost = 0.00055272.\n",
      "Gradient descendent iteration 176. Cost = 0.00053541.\n",
      "Gradient descendent iteration 177. Cost = 0.00051865.\n",
      "Gradient descendent iteration 178. Cost = 0.00050241.\n",
      "Gradient descendent iteration 179. Cost = 0.00048668.\n",
      "Gradient descendent iteration 180. Cost = 0.00047144.\n",
      "Gradient descendent iteration 181. Cost = 0.00045668.\n",
      "Gradient descendent iteration 182. Cost = 0.00044238.\n",
      "Gradient descendent iteration 183. Cost = 0.00042852.\n",
      "Gradient descendent iteration 184. Cost = 0.00041511.\n",
      "Gradient descendent iteration 185. Cost = 0.00040211.\n",
      "Gradient descendent iteration 186. Cost = 0.00038952.\n",
      "Gradient descendent iteration 187. Cost = 0.00037732.\n",
      "Gradient descendent iteration 188. Cost = 0.00036551.\n",
      "Gradient descendent iteration 189. Cost = 0.00035406.\n",
      "Gradient descendent iteration 190. Cost = 0.00034297.\n",
      "Gradient descendent iteration 191. Cost = 0.00033224.\n",
      "Gradient descendent iteration 192. Cost = 0.00032183.\n",
      "Gradient descendent iteration 193. Cost = 0.00031175.\n",
      "Gradient descendent iteration 194. Cost = 0.00030199.\n",
      "Gradient descendent iteration 195. Cost = 0.00029254.\n",
      "Gradient descendent iteration 196. Cost = 0.00028338.\n",
      "Gradient descendent iteration 197. Cost = 0.00027450.\n",
      "Gradient descendent iteration 198. Cost = 0.00026591.\n",
      "Gradient descendent iteration 199. Cost = 0.00025758.\n",
      "Gradient descendent iteration 200. Cost = 0.00024952.\n",
      "Gradient descendent iteration 201. Cost = 0.00024170.\n",
      "Gradient descendent iteration 202. Cost = 0.00023414.\n",
      "Gradient descendent iteration 203. Cost = 0.00022680.\n",
      "Gradient descendent iteration 204. Cost = 0.00021970.\n",
      "Gradient descendent iteration 205. Cost = 0.00021282.\n",
      "Gradient descendent iteration 206. Cost = 0.00020616.\n",
      "Gradient descendent iteration 207. Cost = 0.00019970.\n",
      "Gradient descendent iteration 208. Cost = 0.00019345.\n",
      "Gradient descendent iteration 209. Cost = 0.00018739.\n",
      "Gradient descendent iteration 210. Cost = 0.00018153.\n",
      "Gradient descendent iteration 211. Cost = 0.00017584.\n",
      "Gradient descendent iteration 212. Cost = 0.00017034.\n",
      "Gradient descendent iteration 213. Cost = 0.00016500.\n",
      "Gradient descendent iteration 214. Cost = 0.00015984.\n",
      "Gradient descendent iteration 215. Cost = 0.00015483.\n",
      "Gradient descendent iteration 216. Cost = 0.00014998.\n",
      "Gradient descendent iteration 217. Cost = 0.00014529.\n",
      "Gradient descendent iteration 218. Cost = 0.00014074.\n",
      "Gradient descendent iteration 219. Cost = 0.00013633.\n",
      "Gradient descendent iteration 220. Cost = 0.00013206.\n",
      "Gradient descendent iteration 221. Cost = 0.00012793.\n",
      "Gradient descendent iteration 222. Cost = 0.00012392.\n",
      "Gradient descendent iteration 223. Cost = 0.00012004.\n",
      "Gradient descendent iteration 224. Cost = 0.00011628.\n",
      "Gradient descendent iteration 225. Cost = 0.00011264.\n",
      "Gradient descendent iteration 226. Cost = 0.00010911.\n",
      "Gradient descendent iteration 227. Cost = 0.00010570.\n",
      "Gradient descendent iteration 228. Cost = 0.00010239.\n",
      "Gradient descendent iteration 229. Cost = 0.00009918.\n",
      "Gradient descendent iteration 230. Cost = 0.00009608.\n",
      "Gradient descendent iteration 231. Cost = 0.00009307.\n",
      "Gradient descendent iteration 232. Cost = 0.00009015.\n",
      "Gradient descendent iteration 233. Cost = 0.00008733.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descendent iteration 234. Cost = 0.00008460.\n",
      "Gradient descendent iteration 235. Cost = 0.00008195.\n",
      "Gradient descendent iteration 236. Cost = 0.00007938.\n",
      "Gradient descendent iteration 237. Cost = 0.00007690.\n",
      "Gradient descendent iteration 238. Cost = 0.00007449.\n",
      "Gradient descendent iteration 239. Cost = 0.00007216.\n",
      "Gradient descendent iteration 240. Cost = 0.00006990.\n",
      "Gradient descendent iteration 241. Cost = 0.00006771.\n",
      "Gradient descendent iteration 242. Cost = 0.00006559.\n",
      "Gradient descendent iteration 243. Cost = 0.00006353.\n",
      "Gradient descendent iteration 244. Cost = 0.00006154.\n",
      "Gradient descendent iteration 245. Cost = 0.00005962.\n",
      "Gradient descendent iteration 246. Cost = 0.00005775.\n",
      "Gradient descendent iteration 247. Cost = 0.00005594.\n",
      "Gradient descendent iteration 248. Cost = 0.00005419.\n",
      "Gradient descendent iteration 249. Cost = 0.00005249.\n",
      "Gradient descendent iteration 250. Cost = 0.00005085.\n",
      "Gradient descendent iteration 251. Cost = 0.00004926.\n",
      "Gradient descendent iteration 252. Cost = 0.00004772.\n",
      "Gradient descendent iteration 253. Cost = 0.00004622.\n",
      "Gradient descendent iteration 254. Cost = 0.00004477.\n",
      "Gradient descendent iteration 255. Cost = 0.00004337.\n",
      "Gradient descendent iteration 256. Cost = 0.00004201.\n",
      "Gradient descendent iteration 257. Cost = 0.00004070.\n",
      "Gradient descendent iteration 258. Cost = 0.00003942.\n",
      "Gradient descendent iteration 259. Cost = 0.00003819.\n",
      "Gradient descendent iteration 260. Cost = 0.00003699.\n",
      "Gradient descendent iteration 261. Cost = 0.00003584.\n",
      "Gradient descendent iteration 262. Cost = 0.00003471.\n",
      "Gradient descendent iteration 263. Cost = 0.00003363.\n",
      "Gradient descendent iteration 264. Cost = 0.00003257.\n",
      "Gradient descendent iteration 265. Cost = 0.00003155.\n",
      "Gradient descendent iteration 266. Cost = 0.00003057.\n",
      "Gradient descendent iteration 267. Cost = 0.00002961.\n",
      "Gradient descendent iteration 268. Cost = 0.00002868.\n",
      "Gradient descendent iteration 269. Cost = 0.00002778.\n",
      "Gradient descendent iteration 270. Cost = 0.00002691.\n",
      "Gradient descendent iteration 271. Cost = 0.00002607.\n",
      "Gradient descendent iteration 272. Cost = 0.00002525.\n",
      "Gradient descendent iteration 273. Cost = 0.00002446.\n",
      "Gradient descendent iteration 274. Cost = 0.00002370.\n",
      "Gradient descendent iteration 275. Cost = 0.00002296.\n",
      "Gradient descendent iteration 276. Cost = 0.00002224.\n",
      "Gradient descendent iteration 277. Cost = 0.00002154.\n",
      "Gradient descendent iteration 278. Cost = 0.00002087.\n",
      "Gradient descendent iteration 279. Cost = 0.00002021.\n",
      "Gradient descendent iteration 280. Cost = 0.00001958.\n",
      "Gradient descendent iteration 281. Cost = 0.00001897.\n",
      "Gradient descendent iteration 282. Cost = 0.00001837.\n",
      "Gradient descendent iteration 283. Cost = 0.00001780.\n",
      "Gradient descendent iteration 284. Cost = 0.00001724.\n",
      "Gradient descendent iteration 285. Cost = 0.00001670.\n",
      "Gradient descendent iteration 286. Cost = 0.00001618.\n",
      "Gradient descendent iteration 287. Cost = 0.00001567.\n",
      "Gradient descendent iteration 288. Cost = 0.00001518.\n",
      "Gradient descendent iteration 289. Cost = 0.00001470.\n",
      "Gradient descendent iteration 290. Cost = 0.00001424.\n",
      "Gradient descendent iteration 291. Cost = 0.00001380.\n",
      "Gradient descendent iteration 292. Cost = 0.00001337.\n",
      "Gradient descendent iteration 293. Cost = 0.00001295.\n",
      "Gradient descendent iteration 294. Cost = 0.00001254.\n",
      "Gradient descendent iteration 295. Cost = 0.00001215.\n",
      "Gradient descendent iteration 296. Cost = 0.00001177.\n",
      "Gradient descendent iteration 297. Cost = 0.00001140.\n",
      "Gradient descendent iteration 298. Cost = 0.00001104.\n",
      "Gradient descendent iteration 299. Cost = 0.00001070.\n",
      "Gradient descendent iteration 300. Cost = 0.00001036.\n",
      "Gradient descendent iteration 301. Cost = 0.00001004.\n",
      "Gradient descendent iteration 302. Cost = 0.00000972.\n",
      "Gradient descendent iteration 303. Cost = 0.00000942.\n",
      "Gradient descendent iteration 304. Cost = 0.00000912.\n",
      "Gradient descendent iteration 305. Cost = 0.00000884.\n",
      "Gradient descendent iteration 306. Cost = 0.00000856.\n",
      "Gradient descendent iteration 307. Cost = 0.00000829.\n",
      "Gradient descendent iteration 308. Cost = 0.00000803.\n",
      "Gradient descendent iteration 309. Cost = 0.00000778.\n",
      "Gradient descendent iteration 310. Cost = 0.00000754.\n",
      "Gradient descendent iteration 311. Cost = 0.00000730.\n",
      "Gradient descendent iteration 312. Cost = 0.00000707.\n",
      "Gradient descendent iteration 313. Cost = 0.00000685.\n",
      "Gradient descendent iteration 314. Cost = 0.00000664.\n",
      "Gradient descendent iteration 315. Cost = 0.00000643.\n",
      "Gradient descendent iteration 316. Cost = 0.00000623.\n",
      "Gradient descendent iteration 317. Cost = 0.00000603.\n",
      "Gradient descendent iteration 318. Cost = 0.00000585.\n",
      "Gradient descendent iteration 319. Cost = 0.00000566.\n",
      "Gradient descendent iteration 320. Cost = 0.00000548.\n",
      "Gradient descendent iteration 321. Cost = 0.00000531.\n",
      "Gradient descendent iteration 322. Cost = 0.00000515.\n",
      "Gradient descendent iteration 323. Cost = 0.00000499.\n",
      "Gradient descendent iteration 324. Cost = 0.00000483.\n",
      "Gradient descendent iteration 325. Cost = 0.00000468.\n",
      "Gradient descendent iteration 326. Cost = 0.00000453.\n",
      "Gradient descendent iteration 327. Cost = 0.00000439.\n",
      "Gradient descendent iteration 328. Cost = 0.00000425.\n",
      "Gradient descendent iteration 329. Cost = 0.00000412.\n",
      "Gradient descendent iteration 330. Cost = 0.00000399.\n",
      "Gradient descendent iteration 331. Cost = 0.00000387.\n",
      "Gradient descendent iteration 332. Cost = 0.00000374.\n",
      "Gradient descendent iteration 333. Cost = 0.00000363.\n",
      "Gradient descendent iteration 334. Cost = 0.00000351.\n",
      "Gradient descendent iteration 335. Cost = 0.00000340.\n",
      "Gradient descendent iteration 336. Cost = 0.00000330.\n",
      "Gradient descendent iteration 337. Cost = 0.00000319.\n",
      "Gradient descendent iteration 338. Cost = 0.00000309.\n",
      "Gradient descendent iteration 339. Cost = 0.00000300.\n",
      "Gradient descendent iteration 340. Cost = 0.00000290.\n",
      "Gradient descendent iteration 341. Cost = 0.00000281.\n",
      "Gradient descendent iteration 342. Cost = 0.00000272.\n",
      "Gradient descendent iteration 343. Cost = 0.00000264.\n",
      "Gradient descendent iteration 344. Cost = 0.00000256.\n",
      "Gradient descendent iteration 345. Cost = 0.00000248.\n",
      "Gradient descendent iteration 346. Cost = 0.00000240.\n",
      "Gradient descendent iteration 347. Cost = 0.00000232.\n",
      "Gradient descendent iteration 348. Cost = 0.00000225.\n",
      "Gradient descendent iteration 349. Cost = 0.00000218.\n",
      "Gradient descendent iteration 350. Cost = 0.00000211.\n",
      "Gradient descendent iteration 351. Cost = 0.00000205.\n",
      "Gradient descendent iteration 352. Cost = 0.00000198.\n",
      "Gradient descendent iteration 353. Cost = 0.00000192.\n",
      "Gradient descendent iteration 354. Cost = 0.00000186.\n",
      "Gradient descendent iteration 355. Cost = 0.00000180.\n",
      "Gradient descendent iteration 356. Cost = 0.00000174.\n",
      "Gradient descendent iteration 357. Cost = 0.00000169.\n",
      "Gradient descendent iteration 358. Cost = 0.00000164.\n",
      "Gradient descendent iteration 359. Cost = 0.00000159.\n",
      "Gradient descendent iteration 360. Cost = 0.00000154.\n",
      "Gradient descendent iteration 361. Cost = 0.00000149.\n",
      "Gradient descendent iteration 362. Cost = 0.00000144.\n",
      "Gradient descendent iteration 363. Cost = 0.00000140.\n",
      "Gradient descendent iteration 364. Cost = 0.00000135.\n",
      "Gradient descendent iteration 365. Cost = 0.00000131.\n",
      "Gradient descendent iteration 366. Cost = 0.00000127.\n",
      "Gradient descendent iteration 367. Cost = 0.00000123.\n",
      "Gradient descendent iteration 368. Cost = 0.00000119.\n",
      "Gradient descendent iteration 369. Cost = 0.00000115.\n",
      "Gradient descendent iteration 370. Cost = 0.00000112.\n",
      "Gradient descendent iteration 371. Cost = 0.00000108.\n",
      "Gradient descendent iteration 372. Cost = 0.00000105.\n",
      "Gradient descendent iteration 373. Cost = 0.00000102.\n",
      "Gradient descendent iteration 374. Cost = 0.00000098.\n",
      "Gradient descendent iteration 375. Cost = 0.00000095.\n",
      "Gradient descendent iteration 376. Cost = 0.00000092.\n",
      "Gradient descendent iteration 377. Cost = 0.00000089.\n",
      "Gradient descendent iteration 378. Cost = 0.00000087.\n",
      "Gradient descendent iteration 379. Cost = 0.00000084.\n",
      "Gradient descendent iteration 380. Cost = 0.00000081.\n",
      "Gradient descendent iteration 381. Cost = 0.00000079.\n",
      "Gradient descendent iteration 382. Cost = 0.00000076.\n",
      "Gradient descendent iteration 383. Cost = 0.00000074.\n",
      "Gradient descendent iteration 384. Cost = 0.00000072.\n",
      "Gradient descendent iteration 385. Cost = 0.00000069.\n",
      "Gradient descendent iteration 386. Cost = 0.00000067.\n",
      "Gradient descendent iteration 387. Cost = 0.00000065.\n",
      "Gradient descendent iteration 388. Cost = 0.00000063.\n",
      "Gradient descendent iteration 389. Cost = 0.00000061.\n",
      "Gradient descendent iteration 390. Cost = 0.00000059.\n",
      "Gradient descendent iteration 391. Cost = 0.00000057.\n",
      "Gradient descendent iteration 392. Cost = 0.00000056.\n",
      "Gradient descendent iteration 393. Cost = 0.00000054.\n",
      "Gradient descendent iteration 394. Cost = 0.00000052.\n",
      "Gradient descendent iteration 395. Cost = 0.00000050.\n",
      "Gradient descendent iteration 396. Cost = 0.00000049.\n",
      "Gradient descendent iteration 397. Cost = 0.00000047.\n",
      "Gradient descendent iteration 398. Cost = 0.00000046.\n",
      "Gradient descendent iteration 399. Cost = 0.00000044.\n",
      "Gradient descendent iteration 400. Cost = 0.00000043.\n",
      "Gradient descendent iteration 401. Cost = 0.00000042.\n",
      "Gradient descendent iteration 402. Cost = 0.00000040.\n",
      "Gradient descendent iteration 403. Cost = 0.00000039.\n",
      "Gradient descendent iteration 404. Cost = 0.00000038.\n",
      "Gradient descendent iteration 405. Cost = 0.00000037.\n",
      "Gradient descendent iteration 406. Cost = 0.00000036.\n",
      "Gradient descendent iteration 407. Cost = 0.00000034.\n",
      "Gradient descendent iteration 408. Cost = 0.00000033.\n",
      "Gradient descendent iteration 409. Cost = 0.00000032.\n",
      "Gradient descendent iteration 410. Cost = 0.00000031.\n",
      "Gradient descendent iteration 411. Cost = 0.00000030.\n",
      "Gradient descendent iteration 412. Cost = 0.00000029.\n",
      "Gradient descendent iteration 413. Cost = 0.00000028.\n",
      "Gradient descendent iteration 414. Cost = 0.00000028.\n",
      "Gradient descendent iteration 415. Cost = 0.00000027.\n",
      "Gradient descendent iteration 416. Cost = 0.00000026.\n",
      "Gradient descendent iteration 417. Cost = 0.00000025.\n",
      "Gradient descendent iteration 418. Cost = 0.00000024.\n",
      "Gradient descendent iteration 419. Cost = 0.00000024.\n",
      "Gradient descendent iteration 420. Cost = 0.00000023.\n",
      "Gradient descendent iteration 421. Cost = 0.00000022.\n",
      "Gradient descendent iteration 422. Cost = 0.00000021.\n",
      "Gradient descendent iteration 423. Cost = 0.00000021.\n",
      "Gradient descendent iteration 424. Cost = 0.00000020.\n",
      "Gradient descendent iteration 425. Cost = 0.00000019.\n",
      "Gradient descendent iteration 426. Cost = 0.00000019.\n",
      "Gradient descendent iteration 427. Cost = 0.00000018.\n",
      "Gradient descendent iteration 428. Cost = 0.00000018.\n",
      "Gradient descendent iteration 429. Cost = 0.00000017.\n",
      "Gradient descendent iteration 430. Cost = 0.00000017.\n",
      "Gradient descendent iteration 431. Cost = 0.00000016.\n",
      "Gradient descendent iteration 432. Cost = 0.00000016.\n",
      "Gradient descendent iteration 433. Cost = 0.00000015.\n",
      "Gradient descendent iteration 434. Cost = 0.00000015.\n",
      "Gradient descendent iteration 435. Cost = 0.00000014.\n",
      "Gradient descendent iteration 436. Cost = 0.00000014.\n",
      "Gradient descendent iteration 437. Cost = 0.00000013.\n",
      "Gradient descendent iteration 438. Cost = 0.00000013.\n",
      "Gradient descendent iteration 439. Cost = 0.00000012.\n",
      "Gradient descendent iteration 440. Cost = 0.00000012.\n",
      "Gradient descendent iteration 441. Cost = 0.00000012.\n",
      "Gradient descendent iteration 442. Cost = 0.00000011.\n",
      "Gradient descendent iteration 443. Cost = 0.00000011.\n",
      "Gradient descendent iteration 444. Cost = 0.00000011.\n",
      "Gradient descendent iteration 445. Cost = 0.00000010.\n",
      "Gradient descendent iteration 446. Cost = 0.00000010.\n",
      "Gradient descendent iteration 447. Cost = 0.00000010.\n",
      "Gradient descendent iteration 448. Cost = 0.00000009.\n",
      "Gradient descendent iteration 449. Cost = 0.00000009.\n",
      "Gradient descendent iteration 450. Cost = 0.00000009.\n",
      "Gradient descendent iteration 451. Cost = 0.00000008.\n",
      "Gradient descendent iteration 452. Cost = 0.00000008.\n",
      "Gradient descendent iteration 453. Cost = 0.00000008.\n",
      "Gradient descendent iteration 454. Cost = 0.00000008.\n",
      "Gradient descendent iteration 455. Cost = 0.00000007.\n",
      "Gradient descendent iteration 456. Cost = 0.00000007.\n",
      "Gradient descendent iteration 457. Cost = 0.00000007.\n",
      "Gradient descendent iteration 458. Cost = 0.00000007.\n",
      "Gradient descendent iteration 459. Cost = 0.00000007.\n",
      "Gradient descendent iteration 460. Cost = 0.00000006.\n",
      "Gradient descendent iteration 461. Cost = 0.00000006.\n",
      "Gradient descendent iteration 462. Cost = 0.00000006.\n",
      "Gradient descendent iteration 463. Cost = 0.00000006.\n",
      "Gradient descendent iteration 464. Cost = 0.00000006.\n",
      "Gradient descendent iteration 465. Cost = 0.00000005.\n",
      "Gradient descendent iteration 466. Cost = 0.00000005.\n",
      "Gradient descendent iteration 467. Cost = 0.00000005.\n",
      "Gradient descendent iteration 468. Cost = 0.00000005.\n",
      "Gradient descendent iteration 469. Cost = 0.00000005.\n",
      "Gradient descendent iteration 470. Cost = 0.00000005.\n",
      "Gradient descendent iteration 471. Cost = 0.00000004.\n",
      "Gradient descendent iteration 472. Cost = 0.00000004.\n",
      "Gradient descendent iteration 473. Cost = 0.00000004.\n",
      "Gradient descendent iteration 474. Cost = 0.00000004.\n",
      "Gradient descendent iteration 475. Cost = 0.00000004.\n",
      "Gradient descendent iteration 476. Cost = 0.00000004.\n",
      "Gradient descendent iteration 477. Cost = 0.00000004.\n",
      "Gradient descendent iteration 478. Cost = 0.00000004.\n",
      "Gradient descendent iteration 479. Cost = 0.00000003.\n",
      "Gradient descendent iteration 480. Cost = 0.00000003.\n",
      "Gradient descendent iteration 481. Cost = 0.00000003.\n",
      "Gradient descendent iteration 482. Cost = 0.00000003.\n",
      "Gradient descendent iteration 483. Cost = 0.00000003.\n",
      "Gradient descendent iteration 484. Cost = 0.00000003.\n",
      "Gradient descendent iteration 485. Cost = 0.00000003.\n",
      "Gradient descendent iteration 486. Cost = 0.00000003.\n",
      "Gradient descendent iteration 487. Cost = 0.00000003.\n",
      "Gradient descendent iteration 488. Cost = 0.00000003.\n",
      "Gradient descendent iteration 489. Cost = 0.00000003.\n",
      "Gradient descendent iteration 490. Cost = 0.00000002.\n",
      "Gradient descendent iteration 491. Cost = 0.00000002.\n",
      "Gradient descendent iteration 492. Cost = 0.00000002.\n",
      "Gradient descendent iteration 493. Cost = 0.00000002.\n",
      "Gradient descendent iteration 494. Cost = 0.00000002.\n",
      "Gradient descendent iteration 495. Cost = 0.00000002.\n",
      "Gradient descendent iteration 496. Cost = 0.00000002.\n",
      "Gradient descendent iteration 497. Cost = 0.00000002.\n",
      "Gradient descendent iteration 498. Cost = 0.00000002.\n",
      "Gradient descendent iteration 499. Cost = 0.00000002.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Coeficientes:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "matrix([[1.00005603],\n",
       "        [1.00005603]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Bias:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.999667354496315"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# multivariada\n",
    "x2 = np.matrix('1 1; 2 2; 3 3; 4 4')\n",
    "y2 = np.array([3, 5, 7, 9]).reshape(4,1)\n",
    "\n",
    "weights2, b2 = batch_gradient_descent(x2, y2, alpha=0.05, iterations=500)\n",
    "display(\"Coeficientes:\")\n",
    "display(weights2)\n",
    "display(\"Bias:\")\n",
    "display(b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Aplicação em caso real: Boston House Pricing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando uma classe wrapper para facilitar o uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Facens_LinearRegression:\n",
    "    def __init__(self, learning_rate=0.005, epochs=50):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        (w,b) = batch_gradient_descent(X, Y, \n",
    "                                       alpha=self.learning_rate, \n",
    "                                       iterations=self.epochs)\n",
    "        \n",
    "        self.weights = w\n",
    "        self.bias = b\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return predict(X, self.weights, self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Importando os dados:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os atributos independentes do conjunto X são (nesta ordem):\n",
    "- *CRIM* per capita crime rate by town\n",
    "- *ZN* proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- *INDUS* proportion of non-retail business acres per town\n",
    "- *CHAS* Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "- *NOX* nitric oxides concentration (parts per 10 million)\n",
    "- *RM* average number of rooms per dwelling\n",
    "- *AGE* proportion of owner-occupied units built prior to 1940\n",
    "- *DIS* weighted distances to five Boston employment centres\n",
    "- *RAD* index of accessibility to radial highways\n",
    "- *TAX* full-value property-tax rate per 10000 USD\n",
    "- *PTRATIO* pupil-teacher ratio by town\n",
    "- *B* 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "- *LSTAT* % lower status of the population\n",
    "        \n",
    "O atributo alvo Y é:\n",
    "\n",
    "- *MEDV* Median value of owner-occupied homes in 1000’s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = load_boston(return_X_y=True)\n",
    "Y = Y.reshape(len(Y),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Primeira tentativa de regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "flr = Facens_LinearRegression(learning_rate=0.005, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descendent iteration 0. Cost = 8040154854131.18750000.\n",
      "Gradient descendent iteration 1. Cost = 78727452724886896640.00000000.\n",
      "Gradient descendent iteration 2. Cost = 770882574125724857127665664.00000000.\n",
      "Gradient descendent iteration 3. Cost = 7548319224983011329889724943826944.00000000.\n",
      "Gradient descendent iteration 4. Cost = 73911546369892273738287483305867608064000.00000000.\n"
     ]
    }
   ],
   "source": [
    "flr.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Segunda tentativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descendent iteration 0. Cost = 472.91168984.\n",
      "Gradient descendent iteration 1. Cost = 379.25803773.\n",
      "Gradient descendent iteration 2. Cost = 310.70192918.\n",
      "Gradient descendent iteration 3. Cost = 256.32487619.\n",
      "Gradient descendent iteration 4. Cost = 212.48567420.\n"
     ]
    }
   ],
   "source": [
    "flr2 = Facens_LinearRegression(learning_rate=0.05, epochs=5)\n",
    "flr2.fit(Xs, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando as predições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = flr2.predict(Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame({\"Valor Real\":Y.reshape(len(Y)), \"Predição\":ypred.reshape(len(ypred))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"Resíduo\"] = res[\"Valor Real\"] - res[\"Predição\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Residual Plot')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.scatterplot(data=res, x=\"Valor Real\", y=\"Resíduo\")\n",
    "ax.set_title(\"Residual Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
