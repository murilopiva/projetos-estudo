{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Implementação da classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_Facens():\n",
    "    \n",
    "    #-------------------------------------------------------------------------\n",
    "    # PARTE 1 - Feed Forward\n",
    "    #-------------------------------------------------------------------------\n",
    "    \n",
    "    '''\n",
    "    Inicializa um modelo com hyper-parametros básicos.\n",
    "    \n",
    "    Entradas:\n",
    "        @hidden_layer_sizes\n",
    "            Uma lista cujo tamanho é o número de camadas internas\n",
    "            e os elementos representam o número de neurônios respec\n",
    "            tivos a cada camada.\n",
    "            Exemplos: \n",
    "                [100, 50] = Duas camadas intermediárias, a primeira \n",
    "                com 100 e a segunda com 50 neurônios.\n",
    "\n",
    "                [20] = Uma única camada intermediária com 20 neurô\n",
    "                nios.\n",
    "                \n",
    "        @learning_rate\n",
    "            Fator que regula a velocidade de aprendizado, ou o tamanho\n",
    "            do ajuste nos pesos a cada iteração do gradiente.\n",
    "            \n",
    "        @epochs\n",
    "            Número de iterações do gradiente em batch.\n",
    "            \n",
    "        @hidden_layer_activation\n",
    "            Determina a função de ativação usada nas camadas internas \n",
    "            da rede, usando a função sigmoid (logistíca) como padrão.\n",
    "            Para a camada de saída é aplicado, por padrão, a função \n",
    "            sigmoid a problemas binários e softmax a problemas multi-\n",
    "            classe.\n",
    "            \n",
    "        @leaky_relu_factor\n",
    "            Aplicável apenas quando hidden_layer_activation=LeakyReLU\n",
    "            \n",
    "    '''\n",
    "    def __init__(self, hidden_layer_sizes=[100], #100 neuronios, camada unica | [100,50,10] --> exemplo: 3 camadas intermediarias\n",
    "                 learning_rate=0.001, #quanto mais alto, mais agressiva a atualização; mais baixo, mais de boinha kk\n",
    "                 epochs=200,\n",
    "                 hidden_layer_activation=\"Sigmoid\",\n",
    "                 leaky_relu_factor=0.01):\n",
    "        \n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.hidden_layer_activation = hidden_layer_activation\n",
    "        self.leaky_relu_factor = leaky_relu_factor\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Inicializa os pesos com valores aleatoreos entre 0 e 0.5.\n",
    "    Entradas:\n",
    "        @n\n",
    "            Número de atributos nos dados de treino.\n",
    "            \n",
    "    Saidas:\n",
    "        @self.parameters\n",
    "            variável membro da classe onde são guardados os pesos e \n",
    "            bias para cada uma das camadas.\n",
    "    '''\n",
    "    def _initialize_weights(self, n):\n",
    "        p = {}\n",
    "        L = len(self.hidden_layer_sizes)\n",
    "\n",
    "        previous_size = n\n",
    "        for l in range(1, L+1):\n",
    "            current_size = self.hidden_layer_sizes[l-1] # PREENCHER\n",
    "            #Inicia os valores entre 0 e 0.5, cujo tamanho eh (n0,n1) neuronios da camada input=n0 e camanda hidden=n1\n",
    "            p['W' + str(l)] = np.random.uniform(low=0.0, high=0.5, size=(previous_size,current_size)) # PREENCHER\n",
    "            #Inicia os valores entre 0 e 0.5, de tamanho 1 x tamanho da camada hidden n1\n",
    "            p['b' + str(l)] = np.random.uniform(low=0.0, high=0.5, size=(1,current_size)) # PREENCHER\n",
    "            previous_size = current_size # PREENCHER\n",
    "\n",
    "        #Inicia os valores entre 0 e 0.5, cujo tamanho eh (n1,n2) neuronios da camada hidden=n1 e camada output=n2\n",
    "        p['WOut'] = np.random.uniform(low=0.0, high=0.5, size=(current_size,1)) # PREENCHER\n",
    "        #Inicia os valores entre 0 e 0.5, de tamanho 1 x tamanho da camada output n2\n",
    "        p['bOut'] = np.random.uniform(low=0.0, high=0.5, size=(1,1)) # PREENCHER\n",
    "            \n",
    "        self.parameters = p\n",
    "        return\n",
    "\n",
    "\n",
    "    '''\n",
    "    Dada a consolidação linear dos atributos em z, \n",
    "    esta função calcula a ativação do neurônio se-\n",
    "    gundo a fórmula definida em f.\n",
    "    Entradas:\n",
    "        @z\n",
    "            z = WX + B\n",
    "        @f\n",
    "            Tipo de função de ativação: Sigmoid (logística), ReLU etc. \n",
    "    '''\n",
    "    #Formulas no slide 40\n",
    "    def _activation(self, z, f=\"Sigmoid\"):\n",
    "        \n",
    "        if f == \"Sigmoid\": #função logistica 1 / 1 + ..., slide 40 tbm acho\n",
    "            # PREENCHER\n",
    "            # ----------------------------\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "            # ----------------------------\n",
    "        \n",
    "        elif f == \"ReLU\": ##slide 40\n",
    "            # PREENCHER\n",
    "            # Dica: para definir esta função\n",
    "            # são necessários dois passos:\n",
    "            #  1. Definir a função por cálculo lambda\n",
    "            #  2. Vetorizar o cálculo com np.vectorize\n",
    "            function = lambda t: 0 if t<0 else t\n",
    "            vlfunc = np.vectorize(function)\n",
    "            \n",
    "            # ----------------------------\n",
    "            return vlfunc(z)\n",
    "            # ----------------------------\n",
    "        \n",
    "        elif f == \"LeakyReLU\": #feita pelo prof (pode usar pra preencher a reLu também)\n",
    "            factor  = self.leaky_relu_factor\n",
    "            function = lambda t: t*factor if t<0 else t # PREENCHER (prof disse que a ReLu é igual essa linha só que o t*factor muda alguma coisa parece)\n",
    "            \n",
    "            vfunc = np.vectorize(function)\n",
    "            return vfunc(z)\n",
    "       \n",
    "\n",
    "    '''\n",
    "    Propaga os valores de entrada dos atributos até a saída,\n",
    "    fornendo uma predição para cada registro baseado nos pesos\n",
    "    e nos bias. \n",
    "    Entradas:\n",
    "        @X\n",
    "            Conjunto de valores para os atributos de entrada,\n",
    "            com m linhas e n colunas.\n",
    "    Saidas:\n",
    "        @yhat_out\n",
    "            Vetor com predições para cada um dos m registros.\n",
    "    '''\n",
    "    def feedforward(self, X): #slide 35\n",
    "        self.parameters['A0'] = X\n",
    "        L = len(self.hidden_layer_sizes) + 1\n",
    "\n",
    "        # Hidden layers\n",
    "        for l in range(1, L): #calculo por camada\n",
    "            # PREENCHER\n",
    "            # Dica: Estas variáveis podem\n",
    "            # ser obtidas no dicionário\n",
    "            # self.parameters\n",
    "            # ----------------------------\n",
    "            \n",
    "            A_previous_h = self.parameters['A' + str(l - 1)]\n",
    "            #Wh = None # acho que é o dic que foi criado na classe\n",
    "            Wh = self.parameters['W' + str(l)]\n",
    "            #bh = None # esse tbm é o dic\n",
    "            bh = self.parameters['b' + str(l)]\n",
    "            # ----------------------------\n",
    "            \n",
    "            # PREENCHER com a fórmula da \n",
    "            # agregação linear do perceptron\n",
    "            # ----------------------------\n",
    "            #z_l = None #Calcula o Z -> laranjinha\n",
    "            z_l = A_previous_h.dot(Wh) + bh\n",
    "            # ----------------------------\n",
    "            \n",
    "            #A elevado a L\n",
    "            yhat_l = self._activation(z_l, f=self.hidden_layer_activation)\n",
    "            \n",
    "            # PREENCHER guardando os valores\n",
    "            # de A e Z computados para a camada\n",
    "            # ----------------------------\n",
    "            self.parameters['A' + str(l)] = yhat_l\n",
    "            self.parameters['Z' + str(l)] = z_l\n",
    "            # ----------------------------\n",
    "        \n",
    "        # Output layer\n",
    "        A = self.parameters['A' + str(l)]\n",
    "        W = self.parameters['WOut']\n",
    "        b = self.parameters['bOut']\n",
    "        \n",
    "        z_out = A.dot(W) + b \n",
    "        \n",
    "        yhat_out = self._activation(z_out, f=\"Sigmoid\")\n",
    "        \n",
    "        self.parameters['AOut'] = yhat_out\n",
    "        self.parameters['ZOut'] = z_out\n",
    "\n",
    "        return yhat_out\n",
    "      \n",
    "    '''\n",
    "    Converte as saídas decimais da rede em classes binária.\n",
    "    Entradas:\n",
    "        @yhat_dec\n",
    "            Predições da rede, em formato decimal.\n",
    "    Saidas:     \n",
    "        @y_pred\n",
    "            Predições da rede, em formato binário.\n",
    "    '''\n",
    "    def _decision_threshold(self, yhat_dec):\n",
    "        threshold = 0.5\n",
    "        y_pred = (yhat_dec > threshold) * 1\n",
    "        return y_pred\n",
    "\n",
    "    '''\n",
    "    Usa os pesos calibrados durante o processo de treino para\n",
    "    predizer novas amostras.\n",
    "    Entradas:\n",
    "        @x\n",
    "            Matriz com registros a serem previstos. Deve apre-\n",
    "            sentar os mesmos n atributos usados no treino, sob\n",
    "            o mesmo pré-processamento.\n",
    "    Saidas:     \n",
    "        @yhat\n",
    "            Predições da rede, em formato binário.\n",
    "    '''\n",
    "    def predict(self, x):\n",
    "        yhat = self._decision_threshold(self.feedforward(x))\n",
    "        return yhat\n",
    "    \n",
    "\n",
    "    \n",
    "    #-------------------------------------------------------------------------\n",
    "    # PARTE 2 - Back Propagation\n",
    "    #-------------------------------------------------------------------------\n",
    "    \n",
    "    '''\n",
    "    Função que calcula o custo (erro) do modelo baseada\n",
    "    em entropia cruzada. \n",
    "    Entradas:\n",
    "        @y\n",
    "            Valor real (classe) dos registros.\n",
    "        @yhat\n",
    "            Valor previsto pelo modelo para os registros.\n",
    "    Saidas:\n",
    "        @J\n",
    "            Custo do modelo, indicando quão próximas do\n",
    "            real foram as predições.\n",
    "    '''\n",
    "    def _cost_function(self, y, yhat):\n",
    "        m = len(y)\n",
    "        \n",
    "        yhat = np.where(yhat==0, 0.000000000000001, yhat)\n",
    "        yhat = np.where(yhat==1, 0.999999999999999, yhat)\n",
    "            \n",
    "        t1 = np.multiply(y, np.log(yhat))\n",
    "        t2 = np.multiply((1.0 - y), np.log(1.0 - yhat))\n",
    "        \n",
    "        J = np.sum(-(t1 + t2))/m\n",
    "        return J\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Calcula as derivadas da função de ativação, para cada ponto.\n",
    "    Entradas:\n",
    "        @x\n",
    "            Dominio de f(x), valores usados na entrada de f.\n",
    "        @y\n",
    "            Imagem de f(x), valores de saída de f.\n",
    "        @f\n",
    "            Tipo de função de ativação: Sigmoid (logística), ReLU etc. \n",
    "    '''\n",
    "    def _activation_derivative(self, x, y, f=\"Sigmoid\"):\n",
    "        \n",
    "        #OLHAR A SEGUNDA COLUNA DO SLIDE 40, DERIVATIVE, PRA FAZER AS EQUAÇÕES ABAIXO\n",
    "        if f == \"Sigmoid\":\n",
    "            # PREENCHER\n",
    "            # ----------------------------\n",
    "            return np.multiply(y, (1 - y)) # y * (1-y),. terceira fórmula\n",
    "            # ----------------------------\n",
    "        \n",
    "        elif f == \"ReLU\": #acho que é a sexta formula\n",
    "            # PREENCHER\n",
    "            # Dica: para definir esta função\n",
    "            # são necessários dois passos:\n",
    "            #  1. Definir a função por cálculo lambda\n",
    "            #  2. Vetorizar o cálculo com np.vectorize\n",
    "            # ----------------------------\n",
    "            function = lambda t: 0 if t<0 else 1\n",
    "            vlfunc = np.vectorize(function)\n",
    "            \n",
    "            # ----------------------------\n",
    "            return vlfunc(x)\n",
    "            # ----------------------------\n",
    "        \n",
    "        elif f == \"LeakyReLU\": #usar na ReLu tbm\n",
    "            factor  = self.leaky_relu_factor\n",
    "            function = lambda t: factor if t<0 else 1 # PREENCHER\n",
    "            \n",
    "            vfunc = np.vectorize(function)\n",
    "            return vfunc(x)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Calcula a derivada da função de custo para cada registro.\n",
    "    Entradas:\n",
    "        @y\n",
    "            Valor real (classe) dos registros.\n",
    "        @yhat\n",
    "            Valor previsto pelo modelo para os registros.\n",
    "    '''\n",
    "    def _cost_derivative(self, y, yhat, f=\"CrossEntropy\"):\n",
    "        \n",
    "        if f == \"CrossEntropy\":\n",
    "            a = np.where(yhat==0, 0.000000000000001, yhat)\n",
    "            a = np.where(a==1,    0.999999999999999, a)\n",
    "            \n",
    "            function = lambda y,a: -(y/a) + (1-y)/(1-a)\n",
    "            dA_func = np.vectorize(function)\n",
    "            \n",
    "        return dA_func(y, a)\n",
    "            \n",
    "    '''\n",
    "    Executa uma rodada de atualização dos pesos da rede,\n",
    "    para todas as camadas.\n",
    "    Entradas:\n",
    "        @y\n",
    "            Valor real (classe) dos registros.\n",
    "        @yhat\n",
    "            Valor previsto pelo modelo para os registros.\n",
    "    Saidas:     \n",
    "        @self.parameters\n",
    "            Atualiza os parâmetros diretamente na variável \n",
    "            interna self.parameters.\n",
    "    '''\n",
    "    def backpropagation(self, yhat, y):\n",
    "        m = len(y)\n",
    "        L = len(self.hidden_layer_sizes)\n",
    "    \n",
    "        # Output layer\n",
    "        #------------\n",
    "        A = yhat\n",
    "        \n",
    "        A_previous = self.parameters['A' + str(L)]\n",
    "        Z =  self.parameters['ZOut']\n",
    "        W =  self.parameters['WOut']\n",
    "        \n",
    "        ghat = self._activation_derivative(x=Z, y=yhat, f=\"Sigmoid\")\n",
    "        dA = self._cost_derivative(y, yhat, f=\"CrossEntropy\")\n",
    "   \n",
    "        dZ =  dA * ghat #SLIDE 50, EQUAÇÃO LARANJINHA\n",
    "        dW = (1/m) * A_previous.T.dot(dZ) # a partir do dZ, vê quanto contribuiu\n",
    "        dB = (1/m) * np.sum(dZ,axis=0,keepdims=True)\n",
    "        dA_previous =  dZ.dot(W.T)    \n",
    "        \n",
    "        self.parameters['WOut'] -= self.learning_rate * dW\n",
    "        self.parameters['bOut'] -= self.learning_rate * dB#até aqui é o slide 50\n",
    "\n",
    "        \n",
    "        # Hidden layers\n",
    "        #------------\n",
    "        for l in reversed(range(1, L+1)): # falando disso no video às 16:20\n",
    "            A_previous = self.parameters['A' + str(l - 1)]\n",
    "            A = self.parameters['A' + str(l)]\n",
    "            Z = self.parameters['Z' + str(l)]\n",
    "            W = self.parameters['W' + str(l)]\n",
    "            \n",
    "            ghat = self._activation_derivative(x=Z, y=A, f=self.hidden_layer_activation)\n",
    "            \n",
    "            dA = dA_previous\n",
    "            dZ =  dA * ghat\n",
    "            \n",
    "            # PREENCHER com as derivadas \n",
    "            # parciais\n",
    "            # ----------------------------\n",
    "            dW = (1/m) * A_previous.T.dot(dZ) #só olhar na parte de cima, só mudando a camada\n",
    "            dB = (1/m) * np.sum(dZ,axis=0, keepdims=True) #olhar ali em cima, antes do for tbm\n",
    "            dA_previous = dZ.dot(W.T)           # esse tbm\n",
    "            # ----------------------------\n",
    "            \n",
    "            # PREENCHER com a fórmula de\n",
    "            # atualização dos pesos e bias\n",
    "            # ----------------------------\n",
    "            self.parameters['W' + str(l)] -= self.learning_rate * dW #olhar ali em cima antes do for, pra camada de saida\n",
    "            self.parameters['b' + str(l)] -= self.learning_rate * dB\n",
    "            \n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Para fins de debug, exibe os pesos correntes da rede.\n",
    "    '''\n",
    "    def _print_parameters(self):\n",
    "        L = len(self.hidden_layer_sizes) + 1\n",
    "        \n",
    "        print(\"Layer Output\")\n",
    "        print(self.parameters['WOut'])\n",
    "        print(self.parameters['bOut'])\n",
    "        \n",
    "        for l in reversed(range(1, L)):\n",
    "            print(\"Layer %d\" %(l))\n",
    "            print(self.parameters['W' + str(l)])\n",
    "            print(self.parameters['b' + str(l)])\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Calibra os parâmetros da rede para os dados fornecidos,\n",
    "    executando rodadas de propagação (feedforward) e retro-\n",
    "    propagação (backpropagation) por iterações definadas em\n",
    "    epochs.\n",
    "    Entradas:\n",
    "        @x\n",
    "            Matriz de treino, com m exemplos e n atributos\n",
    "        @y\n",
    "            Atributo alvo z ser previsto.\n",
    "    Saidas:     \n",
    "        @self.parameters\n",
    "            Atualiza os parâmetros diretamente na variável \n",
    "            interna self.parameters.\n",
    "    '''\n",
    "    def fit(self, x, y):\n",
    "        (m, n) = x.shape\n",
    "        self._initialize_weights(n)\n",
    "\n",
    "        cost = 0 \n",
    "        for epoch in range(0, self.epochs):\n",
    "            # PREENCHER com os outputs do\n",
    "            # MLP.\n",
    "            # ----------------------------\n",
    "            yhat = self.feedforward(x) # função pra propagar\n",
    "            # ----------------------------\n",
    "            \n",
    "            cost = self._cost_function(y, yhat)\n",
    "            \n",
    "            if (self.epochs > 100):\n",
    "                if (self.epochs > 1000):\n",
    "                    if (epoch % 100 == 0):\n",
    "                        print(\"*INFO Epoch %d, cost = %.8f\" %(epoch, cost))\n",
    "                else:\n",
    "                    if (epoch % 10 == 0):\n",
    "                        print(\"*INFO Epoch %d, cost = %.8f\" %(epoch, cost))\n",
    "            else:\n",
    "                print(\"*INFO Epoch %d, cost = %.8f\" %(epoch, cost))\n",
    " \n",
    "            # PREENCHER: Qual procedimento está faltando?\n",
    "            # ----------------------------\n",
    "            self.backpropagation(yhat,y) # função pra corrigir os erros\n",
    "            # ----------------------------\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Testes e aplicações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dataset: Ocupação de ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/brvnl/MultilayerPerceptron/master/datatraining.txt\")\n",
    "\n",
    "X2 = df[[\"Temperature\", \"Humidity\", \"Light\", \"CO2\", \"HumidityRatio\"]].copy()\n",
    "Y2 = df[\"Occupancy\"].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "numerical_columns = [\"Temperature\", \"Humidity\", \"Light\", \"CO2\", \"HumidityRatio\"]\n",
    "X2[numerical_columns] = scaler.fit_transform(X2[numerical_columns])\n",
    "\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split(X2, \n",
    "                                                    Y2, \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = MLP_Facens(hidden_layer_sizes=[100],\n",
    "                   learning_rate=0.5, \n",
    "                   epochs=200,\n",
    "                   hidden_layer_activation=\"LeakyReLU\")\n",
    "                   #hidden_layer_activation=\"Sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*INFO Epoch 0, cost = 10.22814485\n",
      "*INFO Epoch 10, cost = 0.79203324\n",
      "*INFO Epoch 20, cost = 0.47647558\n",
      "*INFO Epoch 30, cost = 0.26151456\n",
      "*INFO Epoch 40, cost = 0.12989133\n",
      "*INFO Epoch 50, cost = 0.11475066\n",
      "*INFO Epoch 60, cost = 0.10416449\n",
      "*INFO Epoch 70, cost = 0.09627376\n",
      "*INFO Epoch 80, cost = 0.09014958\n",
      "*INFO Epoch 90, cost = 0.08525222\n",
      "*INFO Epoch 100, cost = 0.08124771\n",
      "*INFO Epoch 110, cost = 0.07791759\n",
      "*INFO Epoch 120, cost = 0.07511184\n",
      "*INFO Epoch 130, cost = 0.07272317\n",
      "*INFO Epoch 140, cost = 0.07067214\n",
      "*INFO Epoch 150, cost = 0.06889833\n",
      "*INFO Epoch 160, cost = 0.06735471\n",
      "*INFO Epoch 170, cost = 0.06600402\n",
      "*INFO Epoch 180, cost = 0.06481629\n",
      "*INFO Epoch 190, cost = 0.06376714\n"
     ]
    }
   ],
   "source": [
    "model2.fit(X2_train.values,Y2_train.values.reshape((len(Y2_train),1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "yh2 = model2.predict(X2_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2119    0]\n",
      " [ 569    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88      2119\n",
      "           1       0.00      0.00      0.00       569\n",
      "\n",
      "    accuracy                           0.79      2688\n",
      "   macro avg       0.39      0.50      0.44      2688\n",
      "weighted avg       0.62      0.79      0.70      2688\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(Y2_test, yh2))\n",
    "print(classification_report(Y2_test, yh2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4295    0]\n",
      " [1160    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      1.00      0.88      4295\n",
      "           1       0.00      0.00      0.00      1160\n",
      "\n",
      "    accuracy                           0.79      5455\n",
      "   macro avg       0.39      0.50      0.44      5455\n",
      "weighted avg       0.62      0.79      0.69      5455\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yh2_2 = model2.predict(X2_train) \n",
    "print(confusion_matrix(Y2_train, yh2_2))\n",
    "print(classification_report(Y2_train, yh2_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
