{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    " <img src=\"https://raw.githubusercontent.com/matheusmota/dataviz2018/master/resources/images/logo_facens_pos.png\" width=\"100x\">\n",
    "    <h3>Aprendizado de Máquina, Redes Neurais e Deep Learning</h3>  \n",
    "    <h4>Classificação de cursos Udemy</h4>\n",
    "</div>\n",
    "\n",
    "\n",
    "* **203066**  - Evandro Bertolucci\n",
    "* **110257**  - João Victor Carvalho\n",
    "* **203071**  - Louise Constantino\n",
    "* **203087**  - Luiza Constantino\n",
    "* **203019**  - Murilo Piva\n",
    "* **203263**  - Rafael Henrique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Performance metric\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "df = pd.read_csv(\"udemy_courses.csv\", encoding='latin-1')\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpar coluna URL\n",
    "df['clean_url'] = df['url'].str.replace('https://www.udemy.com/','')\n",
    "df['clean_url'] = df['clean_url'].str.replace('/','')\n",
    "df['clean_url'] = df['clean_url'].str.replace('-',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar coluna juntando as colunas 'Course Title' e 'Clean Url'\n",
    "df['course_url'] = df['course_title'] + ' ' + df['clean_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar novo DataFrame apenas com as colunas necessárias para analise\n",
    "dfCursos = df[['course_title', 'url', 'course_url', 'subject']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para limpar campos\n",
    "def clean_text(text):\n",
    "    # remove barra invertida-apóstrofo\n",
    "    text = re.sub(\"\\'\", \"\", text)\n",
    "    # remove tudo, exceto alfabetos\n",
    "    text = re.sub(\"[^a-zA-Z]\",\" \",text)\n",
    "    # remove os espaços em branco\n",
    "    text = ' '.join(text.split())\n",
    "    # converter texto em minúsculas\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplica a função de limpar o campo na coluna 'Course Url'\n",
    "dfCursos['clean_course'] = dfCursos['course_url'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Luiza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Download do pacote de palavras irrelevantes da biblioteca NLTK\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para remover as palavras irrelevantes\n",
    "def remove_stopwords(text):\n",
    "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
    "    return ' '.join(no_stopword_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removendo as palavras irrelevantes da coluna 'Clean Course'\n",
    "dfCursos['clean_course'] = dfCursos['clean_course'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando vocabulário\n",
    "def palavrasVocabulario(qtdLinhaPorRotulo, dadosUdemy_Tratados):\n",
    "    vocabulario = []\n",
    "    colunas_usadasModelo = ['course_title', 'url']\n",
    "\n",
    "    for subject in dadosUdemy_Tratados['subject'].unique():\n",
    "        for row in dadosUdemy_Tratados[dadosUdemy_Tratados['subject'] == subject]['clean_course'][:qtdLinhaPorRotulo]:\n",
    "            for p in row.lower().split():\n",
    "                p = p.replace(' ','')\n",
    "                if p not in vocabulario:\n",
    "                    vocabulario.append(p)\n",
    "    \n",
    "    return vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora é necessário representar cada frase usando o vocabulário\n",
    "def Gerar_Amostras(dfCursos, vocabulario):\n",
    "    amostras = []\n",
    "    for t in list(dfCursos['clean_course']):\n",
    "        amostra = []\n",
    "        for p in vocabulario:\n",
    "            if p in t.lower().replace('.','').split():\n",
    "                amostra.append(1)\n",
    "            else:\n",
    "                amostra.append(0)\n",
    "\n",
    "        amostras.append(amostra)\n",
    "\n",
    "    return np.array(amostras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vou pegar todas as linhas, len(dtCursos)\n",
    "vocabulario = palavrasVocabulario(len(dfCursos),dfCursos)\n",
    "amostras = Gerar_Amostras(dfCursos, vocabulario)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = ''\n",
    "with open(\"vocabulario.txt\", \"w\") as text_file:\n",
    "    for x in vocabulario:\n",
    "        text_file.write(x + \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dfCursos.iloc[:,2:3].values, dfCursos.iloc[:,3:4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComplementNB()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ComplementNB()\n",
    "model.fit(amostras, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['udemy.joblib']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(model, 'udemy.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vo = []\n",
    "with open(\"vocabulario.txt\", encoding='utf8') as txt:\n",
    "    for x in txt:\n",
    "        li = x.split(',')  \n",
    "        for h in li:\n",
    "            if h not in vo and len(h) > 0:\n",
    "                vo.append(h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
